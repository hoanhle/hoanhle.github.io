<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>From Transformer to ChatGPT | Hoanh Le </title> <meta name="author" content="Hoanh Le "/> <meta name="description" content="going back in time in order to move forward"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg"/> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="hoanhle.github.io/blog/2023/distill/"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}.l-gutter img{width:150%;height:auto;max-width:none;transform:translateY(-30px)}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "From Transformer to ChatGPT",
      "description": "going back in time in order to move forward",
      "published": "November 19, 2023",
      "authors": [
        {
          "author": "Hoanh Le",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Hoanh Le&nbsp;</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/bookshelf/">bookshelf</a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">contact</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>From Transformer to ChatGPT</h1> <p>going back in time in order to move forward</p> </d-title><d-byline></d-byline><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#attention-is-all-you-need">Attention is all you need</a></div> <ul> <li><a href="#attention-mechanism">Attention mechanism</a></li> <li><a href="#multi-head-attention">Multi-head Attention</a></li> <li><a href="#self-attention">Self-attention</a></li> <li><a href="#transformer-encoder">Transformer encoder</a></li> <li><a href="#transformer-decoder">Transformer decoder</a></li> <li><a href="#positional-encoding">Positional encoding</a></li> </ul><div><a href="#improving-language-understanding-by-generative-pre-training-gpt-1">Improving Language Understanding by Generative Pre-Training (GPT-1)</a></div> <ul> <li><a href="#a-new-paradigm">A new paradigm</a></li> <li><a href="#tokenization">Tokenization</a></li> <li><a href="#generative-pre-training">Generative Pre-training</a></li> <li><a href="#discriminative-fine-tuning">Discriminative fine-tuning</a></li> <li><a href="#the-long-term-vision">The long-term vision</a></li> </ul><div><a href="#language-models-are-unsupervised-multitask-learners-gpt-2">Language Models are Unsupervised Multitask Learners (GPT-2)</a></div> <div><a href="#scaling-laws-for-neural-language-models">Scaling Laws for Neural Language Models</a></div> <ul> <li><a href="#transformer-lms-complexity">Transformer LMs complexity</a></li> <li><a href="#transformer-lms-and-orders-of-magnitude">Transformer LMs and Orders of Magnitude</a></li> <li><a href="#scaling-laws">Scaling laws</a></li> </ul><div><a href="#language-models-are-few-shot-learners-gpt-3">Language Models are Few-Shot Learners (GPT-3)</a></div> <div><a href="#instruction-tuning-flan">Instruction Tuning (FLAN)</a></div> <div><a href="#reinforcement-learning-from-human-feedback-rlhf">Reinforcement Learning from Human Feedback (RLHF)</a></div> <div><a href="#training-language-models-to-follow-instructions-with-human-feedback-instruct-gpt">Training Language Models to follow instructions with Human Feedback (Instruct-GPT)</a></div> </nav> </d-contents> <h2 id="attention-is-all-you-need">Attention is all you need</h2> <p>Since their introduction in 2017, transformers <d-cite key="vaswani2023attention"></d-cite> have revolutionised NLP, and now applications over all domains in Deep Learning. Why is this significant: rather than relying on domain-specific architectures, we can now utilize a single architecture for every domain.</p> <p>On a high level: transformers represents an efficient, optimizable, general-purpose neural network. They are designed to encode and decode sequences of data, allowing individual tokens to interact and communicate with one another.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/architecture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/architecture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/architecture-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/architecture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="attention-mechanism">Attention mechanism</h3> <p>The attention mechanism let us lets us model dependencies between words regardless of their position in the sequence. It was first introduced by Bahdanau et al. <d-cite key="bahdanau2016neural"></d-cite> where they found out</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a fixed-length vector is a bottleneck in improving the performance of this 
basic encoder–decoder architecture, and propose to extend this by allowing 
a model to automatically (soft-)search for parts of a source sentence that 
are relevant to predicting a target word, without having to form these 
parts as a hard segment explicitly. 
</code></pre></div></div> <p>Basic attention mechanism in transformers:</p> \[\begin{aligned} o_i &amp; =\sum_{j=1}^n \alpha_{i j} z_j \\ \alpha_{i j} &amp; =\frac{\exp \left(z_j^{\top} h_i / \sqrt{d_k}\right)}{\sum_{j^{\prime}=1}^n \exp \left(z_{j^{\prime}}^{\top} h_i / \sqrt{d_k}\right)} \end{aligned}\] <div class="l-gutter"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/attention_transformer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/attention_transformer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/attention_transformer-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/attention_transformer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>$d_k$ is the dimensionality of the $z_j$ and $h_i$.</p> <p>The authors call this scaled-dot</p> <p>We can think of the scaled dot-product attention as finding values $v_j=z_j$ with keys $k_j=z_j$ that are closest to query $q_i=h_i$. Re-writing the scaled dot-product attention using keys, values and query:</p> \[\begin{aligned} o_i=\sum_{j=1}^n \alpha_{i j} z_j &amp; o_i=\sum_{j=1}^n \alpha_{i j} v_j \\ \\ \alpha_{i j}=\frac{\exp \left(z_j^{\top} h_i / \sqrt{d_k}\right)}{\sum_{j^{\prime}=1}^n \exp \left(z_{j^{\prime}}^{\top} h_i / \sqrt{d_k}\right)} &amp; \alpha_{i j}=\frac{\exp \left(k_j^{\top} q_i / \sqrt{d_k}\right)}{\sum_{j^{\prime}=1}^n \exp \left(k_{j^{\prime}}^{\top} q_i / \sqrt{d_k}\right)} \end{aligned}\] <p>Scaled dot-product attention:</p> \[\begin{aligned} o_i &amp; =\sum_{j=1}^n \alpha_{i j} v_j \\ \alpha_{i j} &amp; =\frac{\exp \left(k_j^{\top} q_i / \sqrt{d_k}\right)}{\sum_{j^{\prime}=1}^n \exp \left(k_{j^{\prime}}^{\top} q_i / \sqrt{d_k}\right)} \end{aligned}\] <div class="l-gutter"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/scaled_dot_product-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/scaled_dot_product-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/scaled_dot_product-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/scaled_dot_product.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>in the matrix form:</p> \[\begin{aligned} &amp; \qquad \operatorname{attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{\top}}{\sqrt{d_k}}\right) V \\ &amp; \text { with } V \in R^{m \times d_v}, Q \in R^{n \times d_k}, K \in R^{m \times d_k} \end{aligned}\] <h3 id="multihead-attention">Multihead attention</h3> <p>Instead of doing a single scaled dot-product attention, the authors found it beneficial to project keys, queries and values into lower-dimensional spaces, perform scaled dot-product attention there and concatenate the outputs</p> \[\begin{gathered} \operatorname{head}_i=\operatorname{attention}\left(Q W_i^Q, K W_i^K, V W_i^V\right) \\ \\ \operatorname{MultiHead}(Q, K, V)=\text { Concat }\left(\operatorname{head}_1, \ldots, \text { head }_h\right) W^O \\ \\ V \in R^{m \times d_v}, Q \in R^{n \times d_k}, K \in R^{m \times d_k} \\ \\ \text { head }_i \in R^{n \times d_i}, \text { output } \in R^{n \times d_k} . \end{gathered}\] <div class="l-gutter"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/multi_head_attention-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/multi_head_attention-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/multi_head_attention-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/multi_head_attention.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="self-attention">Self-Attention</h3> <p>Attention is all you need: Use the same multi-head attention mechanism to convert inputs $\left(\mathbf{x}_1, \ldots, \mathbf{x}_n\right)$ into representations $\left(\mathbf{z}_1, \ldots, \mathbf{z}_n\right)$.</p> <p>For simplicity, assume that we use scaled dot-product attention:</p> \[z_i=\sum_{j=1}^n \alpha_{i j} x_j \quad \alpha_{i j}=\frac{\exp \left(x_j^{\top} x_i / \sqrt{d_k}\right)}{\sum_{j^{\prime}=1}^n \exp \left(x_{j^{\prime}}^{\top} x_i / \sqrt{d_k}\right)}\] <div class="l-gutter"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/self_attention-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/self_attention-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/self_attention-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/self_attention.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Thus, we use vectors $x_i$ as keys, values and queries. This is called self-attention. Advantage: The first position affects the representation in the last position (and vice versa) already after one layer! (think how many layers are needed for that in RNN or convolutional encoders).</p> <h3 id="transformer-encoder">Transformer encoder</h3> <p>After self-attention, the representation in each position is processed with a mini-MLP (Feed Forward block in the figure). The encoder is a stack of multiple such blocks (each block contains an attention module and a mini-MLP).</p> <div class="l-gutter"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_encoder-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_encoder-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_encoder-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/transformer_encoder.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="transformer-decoder">Transformer decoder</h3> <p>When predicting word $y_i$ we can use the preceding words $y_1$, …, $y_{i-1}$ but not subsequent words $y_i$, …, $y_m$. Again, attention-is-all-you-need idea: Use self-attention as a building block of the decoder. We need to make sure that we do not use subsequent inputs $y_i$, …, $y_m$ when producing output $o_i$ at position $i$. This is done using masked self-attention</p> <div class="l-gutter"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_decoder-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_decoder-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_decoder-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/transformer_decoder.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>For simplicity, assume that we use scaled dot-product attention:</p> \[h_i=\sum_{j=1}^m \alpha_{i j} v_j \quad \alpha_{i j}=\frac{\exp \left(v_j^{\top} v_i / \sqrt{d_k}+m_{i j}\right)}{\sum_{j^{\prime}=1}^m \exp \left(v_{j^{\prime}}^{\top} v_i / \sqrt{d_k}+m_{i j^{\prime}}\right)}\] <p>We want not to use subsequent positions</p> <p>\(v_{i+1}, \ldots, v_m\) when computing output $h_i$. We can do that using attention masks $m_{i j}$:</p> \[\begin{aligned} &amp; m_{i j}=0, \text { if } j \leq i \\ &amp; m_{i j}=-\infty \text { and therefore } \alpha_{i j}=0, \text { if } j&gt;i \end{aligned}\] <p>After self-attention and cross-attention, the representation in each position is processed with a mini-MLP (Feed Forward block in the figure). The decoder is a stack of multiple such blocks (each block contains two attention modules and a mini-MLP).</p> <div class="l-gutter"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_decoder_architecture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_decoder_architecture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_decoder_architecture-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/transformer_decoder_architecture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="positional-encoding">Positional encoding</h3> <p>Transformers use hard-coded (not learned) positional encoding:</p> \[\begin{aligned} &amp; PE(p, 2 i) =\sin \left(p / 10000^{2 i / d}\right) \\ &amp; PE(p, 2 i+1) =\cos \left(p / 10000^{2 i / d}\right) \end{aligned}\] <p>where $p$ is position, $i$ is the element of the encoding. This encoding has the same dimensionality $d$ as input/output embeddings. This encoding has the same dimensionality d as input/output embeddings.</p> <h4 id="end-note">End note</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Encoder-decoder architecture
Transformer layers with Multi-Head Self-Attention and Feed-Forward sublayers 
Transformer Decoder layers also have Encoder-Decoder Attention sublayer
N Encoder and Decoder layers (N=6 in the paper)
Attention uses queries and key-value pairs to produce weighted sums of the values 
Feed-Forward layers then process the inputs independently for better representations
Trained end-to-end for translating sequences
Superior performance over RNNs
</code></pre></div></div> <p>If you have trouble understanding the model still, consider looking at this <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">Annotated Transformer</a> blog post.</p> <h2 id="improving-language-understanding-by-generative-pre-training-gpt-1">Improving Language Understanding by Generative Pre-Training (GPT-1)</h2> <p>OpenAI introduced the first version of GPT (Generative Pre-Training) <d-cite key="radford2018improving"></d-cite>, which is a scalable, task-agnostic architecture, achieving state-of-the-art results on multiple diverse tasks. The approach combines two existing ideas: transformers and unsupervised pre-training <d-cite key="dai2015semisupervised"></d-cite>. The results show that pairing supervised learning methods with unsupervised pre-training works very well, and if we improve the performance of the model in the pre-training phase, it might become good enough to not require any fine-tuning on more diverse tasks.</p> <h3 id="introducing-a-new-paradigm">Introducing a new paradigm</h3> <blockquote> <p>Most deep learning methods require substantial amounts of manually labeled data, which restricts their applicability in many domains that suffer from a dearth of annotated resources.</p> </blockquote> <p>Which raises the following questions:</p> <ul> <li>Can we learn from raw text instead of pairs of translated sentences?</li> <li>What objectives should we use?</li> <li>How to apply (or “transfer”) what we learn to the tasks we care?</li> <li>Does it help in practice to model language in order to solve NLP tasks?</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt_new_paradigm-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt_new_paradigm-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt_new_paradigm-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/gpt_new_paradigm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Introducing a new paradigm </div> <h3 id="tokenization">Tokenization</h3> <p>Imagine our corpus has the following words</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"from", "transformer", "to", "chatgpt"
</code></pre></div></div> <p>then our base vocabulary will be</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['a', 'c', 'e', 'f', 'g', 'h', 'm', 'n', 'o', 'p', 'r', 's', 't']
</code></pre></div></div> <p>For real-world use cases, base vocabulary will contain all the ASCII characters, and maybe some Unicode characters as well. If am example you want to tokenize uses a character that is not in the training corpus, that character will be converted to unknown tokens. That’s one reason why lots of NLP model s are bad at analyzing content with emojis, e.g.</p> <p>Good characteristics of good vocabulary</p> <ul> <li>Be able to represent ALL characters and symbols that exist in digital format</li> <li>The vocabulary should be as expressive as possible (e.g. words are more expressive than characters)</li> </ul> <p>Options</p> <ul> <li>Bytes: 2^8=256 possible tokens only, can represent anything, but not expressive</li> <li>Unicode codes (character level tokens): 130k+ tokens, too large!</li> <li>Words: more than 1M only in English vocabularies, too large and not flexible enough!</li> <li>Sub-words: fuzzy term to indicate anything from byte-pairs up to full words; can be crafted to satisfy all the desiderata above!</li> </ul> <p>GPT paper used a technique called <code class="language-plaintext highlighter-rouge">bytepair encoding (BPE) vocabulary with 40,000 merges</code>. It’s a greedy data-compression technique that iterative replaces the most frequent pair of tokens in a sequence with a single unused token.</p> <p>Procedure</p> <ol> <li>Initialize vocabulary V = {all possible bytes}</li> <li>Count the frequency of all unique subsequent pairs of tokens</li> <li>Substitute the most frequent pair with a new token and add the token to V (“merge” op.)</li> <li>Repeat 2. and 3. until the V size is reached or after N merges ops</li> </ol> <details><summary>How to implement BPE</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">Encoder-decoder architecture</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Transformer layers with Multi-Head Self-Attention and Feed-Forward sublayers</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Transformer Decoder layers also have Encoder-Decoder Attention sublayer</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">N Encoder and Decoder layers (N=6 in the paper)</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Attention uses queries and key-value pairs to produce weighted sums of the values</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Feed-Forward layers then process the inputs independently for better representations</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Trained end-to-end for translating sequences</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Superior performance over RNNs</span><span class="sh">'</span><span class="p">,</span>
<span class="p">]</span>

<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>


<span class="k">def</span> <span class="nf">create_base_vocab</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
    <span class="c1"># Procedure 1: Initialize vocabulary V = {all possible bytes}
</span>    <span class="n">base_vocab</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">base_vocab</span><span class="p">:</span>
                <span class="n">base_vocab</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">base_vocab</span>


<span class="k">def</span> <span class="nf">get_word_frequencies</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
    <span class="n">word_freq</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">.</span><span class="nf">split</span><span class="p">():</span>
            <span class="n">word_freq</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">word_freq</span>


<span class="k">def</span> <span class="nf">get_splits</span><span class="p">(</span><span class="n">word_freq</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="p">[</span><span class="n">char</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="nf">keys</span><span class="p">()}</span>


<span class="k">def</span> <span class="nf">find_pair_freqs</span><span class="p">(</span><span class="n">word_freq</span><span class="p">,</span> <span class="n">splits</span><span class="p">):</span>
    <span class="c1"># Procedure 2: Count the frequency of all unique subsequent pairs of tokens
</span>    <span class="n">pair_freq</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">split</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">split</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">pair_freq</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>
    <span class="k">return</span> <span class="n">pair_freq</span>


<span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">splits</span><span class="p">):</span>
    <span class="c1"># Procedure 3: Substitute the most frequent pair with a new token and add the token to V (“merge” op.)
</span>    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
        <span class="n">split</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">split</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">a</span> <span class="ow">and</span> <span class="n">split</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">b</span><span class="p">:</span>
                <span class="n">split</span> <span class="o">=</span> <span class="n">split</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">]</span> <span class="o">+</span> <span class="n">split</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">splits</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">split</span>
    <span class="k">return</span> <span class="n">splits</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">60</span><span class="p">):</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="nf">create_base_vocab</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
    <span class="n">word_freq</span> <span class="o">=</span> <span class="nf">get_word_frequencies</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="nf">get_splits</span><span class="p">(</span><span class="n">word_freq</span><span class="p">)</span>

    <span class="c1"># Procedure 4: Repeat 2. and 3. until V reaches N elements (e.g. 32k)
</span>    <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">vocab_size</span><span class="p">:</span>
        <span class="n">pair_freq</span> <span class="o">=</span> <span class="nf">find_pair_freqs</span><span class="p">(</span><span class="n">word_freq</span><span class="p">,</span> <span class="n">splits</span><span class="p">)</span>
        <span class="n">best_pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_freq</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="nf">merge</span><span class="p">(</span><span class="o">*</span><span class="n">best_pair</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">)</span>
        <span class="n">vocab</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="nf">print</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">main</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span></code></pre></figure> </details> <h3 id="generative-pre-training">Generative Pre-Training</h3> <p>Given an unsupervised corpus of tokens $U=\left{u_1, \ldots, u_n\right}$, they use a standard language modeling objective to maximize the following likelihood:</p> \[L_1(U)=\sum_i \log P\left(u_i \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)\] <p>where $k$ is the size of the context window, and the conditional probability $P$ is modeled using a neural network with parameters $\Theta$.</p> <p>They use Decoder-only Transformer, a variant of Transformer as the base architecture, as its excellent for handling complex dependencies within sequences</p> <p>\(\begin{aligned} h_0 &amp; =U W_e+W_p \\ h_l &amp; =\text { transformer_block }\left(h_{l-1}\right) \forall i \in[1, n] \\ P(u) &amp; =\operatorname{softmax}\left(h_n W_e^T\right) \\ \\ \end{aligned}\) where $U=\left(u_{-k}, \ldots, u_{-1}\right)$ is the context vector of tokens, $n$ is the number of layers, $W_e$ is the token embedding matrix, and $W_p$ is the position embedding matrix.</p> <div class="l-gutter"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/generative_pre_training-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/generative_pre_training-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/generative_pre_training-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/generative_pre_training.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="discriminative-fine-tuning">Discriminative fine-tuning</h3> <div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/discriminative_fine_tuning-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/discriminative_fine_tuning-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/discriminative_fine_tuning-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/discriminative_fine_tuning.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>After pre-training the model, the parameters are adapted to supervised target task. Assume label set $C$, a sequence of input tokens $x^1, \ldots, x^m$, along with a label $y$. The inputs are <code class="language-plaintext highlighter-rouge">first converted into an ordered sequence that our pre-trained model can process, passed through GPT to obtain the hidden representation of the last layer for the last token as the sequence representation, which is then fed into an added linear output layer with parameters $W_y$ to predict $y$</code></p> \[P\left(y \mid x^1, \ldots, x^m\right)=\operatorname{softmax}\left(h_l^m W_y\right) .\] <p>That gives the following language modelling objective to maximize:</p> \[L_2(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^1, \ldots, x^m\right) .\] <p>They found that including language modeling as an auxiliary objective to the fine-tuning helped learning by (a) improving generalization of the supervised model, and (b) accelerating convergence. They optimize the following objective (with weight $\lambda$ ):</p> \[L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda * L_1(\mathcal{C})\] <p><code class="language-plaintext highlighter-rouge">The ony extra parameters required during fine-tuning are $W_y$, and embeddings for delimiter tokens.</code></p> <h3 id="the-long-term-vision">The long-term vision</h3> <div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/zero_shot_performance-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/zero_shot_performance-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/zero_shot_performance-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/zero_shot_performance.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Plot showing the evolution of zero-shot performance on different tasks as a function of LM pre-training updates. Performance per task is normalized between a random guess baseline and the current state-of-the-art with a single model. </div> <p>In order to understand why the pre-training of transformers is so effective, the authors used a series of heuristic solutions that use the underlying generative model only without supervised fine-tuning. The results show the performance is stable and increases over training suggesting that pre-training supports the learning of a wide variety of task relevant functionality. <code class="language-plaintext highlighter-rouge">The vision is if we improve the performance of the model in the pre-training phase, it might become good enough to not require any fine-tuning!</code></p> <h2 id="language-models-are-unsupervised-multitask-learners-gpt-2">Language Models are Unsupervised Multitask Learners (GPT-2)</h2> <p>Hypothesis: Language acts as a versatile medium that encodes tasks, inputs, and outputs into a uniform sequence of symbols. For instance, a translation task can be represented as (translate to French, English text, French text), and similarly, a reading comprehension task as (answer the question, document, question, answer). Given that the supervised learning objective simply narrows down the focus to a particular segment of this sequence, the ultimate goal—achieving the lowest error rate, known as the global minimum—remains the same for both supervised and unsupervised learning objectives. Consequently, a model trained on an unsupervised next-token prediction task inherently strives towards a goal that aligns with that of supervised tasks, suggesting that the model’s pre-training on diverse web data equips it with zero-shot transfer capabilities through an underlying meta-learning process during its generative pre-training.</p> <p><code class="language-plaintext highlighter-rouge">With more than 10X the parameters and trained on more than 10X the amount of data., GPT-2 zero-shots to state of the art performance on 7 out of 8 tested language modeling datasets.</code></p> <div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt2_performance-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt2_performance-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt2_performance-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/gpt2_performance.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> GPT-2 achieves state-of-the-art on Winograd Schema, LAMBADA, and other language modeling tasks. <d-cite key="openai2019better"></d-cite> </div> <div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt2_performance_size-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt2_performance_size-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt2_performance_size-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/gpt2_performance_size.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Zero-shot task performance of WebText LMs as a function of model size on many NLP tasks. <d-cite key="Radford2019LanguageMA"></d-cite> </div> <p>Looking at the above plot, we can clearly see more data + bigger models leads to better zero-shot abilities. And all models still underfit WebText, i.e. if trained for longer would get better test perplexity!</p> <div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt2_underfitting-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt2_underfitting-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt2_underfitting-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/gpt2_underfitting.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> The performance of LMs trained on WebText as a function of model size. <d-cite key="Radford2019LanguageMA"></d-cite> </div> <details><summary>Note on perplexity</summary> <p>In Natural Language Processing (NLP), perplexity is a measurement of how well a probability model predicts a sample. It is commonly used to evaluate language models. Perplexity is defined as the exponentiation of the entropy of the probability distribution for a given test dataset.</p> <p>Here’s a step-by-step breakdown of the calculation of perplexity for a language model:</p> <ol> <li> <p><strong>Probability of the Sequence</strong>: First, compute the probability of the sequence of words (or tokens) according to the language model. This is often done by multiplying the probabilities of each word given the previous words in the sequence.</p> </li> <li> <p><strong>Calculate Entropy</strong>: The entropy of the sequence is the average negative log-probability of the test set sequences. For a sequence of words $W=w_1, w_2, \ldots, w_N$, the entropy $H$ is calculated as: \(H(W)=-\frac{1}{N} \sum_{i=1}^N \log _2 p\left(w_i \mid w_1, w_2, \ldots, w_{i-1}\right)\) where \(p\left(w_i \mid w_1, w_2, \ldots, w_{i-1}\right)\) is the probability of word $ w_i $ given the preceding words in the sequence.</p> </li> <li> <p><strong>Exponentiate the Entropy</strong>: Perplexity $P P$ is defined as the exponentiation of the entropy: \(P P(W)=2^{H(W)}\)</p> </li> </ol> <p>The intuition behind perplexity is that it measures how “perplexed” the model is when predicting new data; a lower perplexity indicates that the model is more confident in its predictions. In other words, a good language model will have a low perplexity, meaning that the sequences it predicts are relatively likely.</p> <p>In this form, you don’t exponentiate the log probabilities but rather average them directly and then apply the exponentiation. This is mathematically equivalent to the original formula but is more practical for computational purposes.</p> </details> <h2 id="scaling-laws-for-neural-language-models">Scaling Laws for Neural Language Models</h2> <p>From the GPT-2 paper, we see that pre-training shows promising result by having more data and scaling up the model. We want to understand:</p> <ul> <li>Q1: What is the limit of this data+model scale effect? How much more can we push it?</li> <li>Q2: How much will it cost us (in terms of GPU-hours and money) to reach a certain performance? What’s the best we can do with a given budget?</li> </ul> <h3 id="transformer-lms-complexity">Transformer LMs complexity</h3> <p>The number of parameters in a decoder-only transformer language is:</p> \[\text { Parameters }=12 n_{\text {layer }} d_{\text {model }}^2\] <p>Roughly speaking, all parameters participate in 1 add and 1 multiply in a forward pass, and 2x in the backward pas, for every token. So training compute is:</p> \[\text { Compute }=6 P D\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_complexity-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_complexity-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_complexity-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/transformer_complexity.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="transformer-lms-and-orders-of-magnitude">Transformer LMs and Orders of Magnitude</h3> <p>The largest models we have so far has roughly in the order of 1 trillion ($1e^12$) parameters.</p> <p>A100 GPUs performs $~3e^14 FLOPs$ or $2e^19 FLOP/day$, a PF-day is around ~ 3 A100 day $~8.6e^19 FLOPs$.</p> <p>Largest Train Compute = $6 P D = 6^* 1 \mathrm{e} 12 * 3 \mathrm{e} 11$ (same number of tokens GPT-3 was trained on)</p> <h3 id="scaling-laws">Scaling laws</h3> <p>There are precise scaling laws for performance of ML models as functions of <code class="language-plaintext highlighter-rouge">model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range</code> <d-cite key="kaplan2020scaling"></d-cite></p> <p>To get scaling results, we can train many models of different sizes on different sized datasets. The performance of language models gets better consistently when you increase the size of the model, the dataset, and the computing power used in training. To get the best results, you need to increase all three together. When one of these factors isn’t limited by the others, its improvement follows a power-law trend.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/scaling_laws-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/scaling_laws-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/scaling_laws-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/scaling_laws.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>You want to spend most of your compute on making the model bigger, around 2/3 on the geometric scale</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/optimal_compute-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/optimal_compute-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/optimal_compute-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/optimal_compute.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Architecture you want to train on is less important, unless the architecture itself creates a bottleneck. You can of course still do language modelling with LSTMs instead of transformer. And you can see what you actually get using LSTMs instead of transformers.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_lstm-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_lstm-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/transformer_lstm-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/transformer_lstm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>At zeroth order, LSTMs didnt seem too bad, and as you make them bigger, they are scaling up quite nicely. But there is a constant offset where transformers are five to ten times more efficient. Moreover, if we take a look at 1000 tokens, we can look at the losses as a function of the position in context. And we can see that LSTMs plateau quite quickly at around 100 tokens.</p> <h2 id="language-models-are-few-shot-learners-gpt-3">Language Models are Few-Shot Learners (GPT-3)</h2> <p>As discussed before, while using task-specific datasets and task-specific fine-tuning can bring strong performance on a desired task, its not the right way forward. Every new task requires a large dataset of labeled examples, which then will limit the applicability of language models. Now, using empirical scaling laws, we want to see how far we can get with task-agnostic architecture with larger model and more data.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt_3_result-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt_3_result-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt_3_result-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/gpt_3_result.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As the results show, we can see that zero-shot performance is still not competitive with fine-tuned models, but By just providing examples of desired input-output behaviour in the prompt to the model, performance increases greatly without doing gradient updates.</p> <p>Introducing the concept of in-context learning: <code class="language-plaintext highlighter-rouge">using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.</code> <d-cite key="brown2020language"></d-cite></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/in_context_learning-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/in_context_learning-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/in_context_learning-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/in_context_learning.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Introducing a new paradigm </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/in_context_learning_settings-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/in_context_learning_settings-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/in_context_learning_settings-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/in_context_learning_settings.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Introducing a new paradigm </div> <p>In their study, the authors trained GPT-3, a language model with 175 billion parameters, to evaluate its ability to learn in-context. They tested GPT-3 on a variety of NLP tasks, as well as various new tasks GPT-3 hadn’t encountered in its training data. They assessed GPT-3 in three scenarios: few-shot learning (using 10 to 100 examples within the model’s context window), one-shot learning (with just one example), and zero-shot learning (without any examples, only instructions).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt_3_performance-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt_3_performance-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/gpt_3_performance-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/gpt_3_performance.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. </div> <h2 id="instruction-tuning-flan">Instruction Tuning (FLAN)</h2> <p>Large-scale language models like GPT-3, developed by Brown and others in 2020, excel at learning from a few examples (few-shot learning). However, they struggle more with learning without any examples (zero-shot learning). For instance, GPT-3 does not perform as well in zero-shot scenarios, especially in tasks like reading comprehension, answering questions, and understanding language inferences, compared to when it has a few examples to learn from. This might be because, without these examples, the models find it difficult to respond to prompts that differ significantly from the data they were trained on.</p> <p>Google Researches proposed an easy approach to enhance the ability of language models to learn without prior examples (zero-shot learning). They demonstrate that instruction tuning <d-cite key="wei2022finetuned"></d-cite>, which involves fine-tuning language models using a variety of datasets described through instructions, significantly boosts their performance in zero-shot scenarios on tasks they haven’t encountered before.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/instruction_tuning_template-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/instruction_tuning_template-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/instruction_tuning_template-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/instruction_tuning_template.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Multiple instruction templates describing a natural language inference task. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/instruction_tuning-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/instruction_tuning-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/instruction_tuning-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/instruction_tuning.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Comparing instruction tuning with pretrain–finetune and prompting. </div> <p>In their research, the authors found that when using the most effective development template, FLAN in a zero-shot setting outperformed GPT-3 in zero-shot learning on 20 out of 25 datasets. Impressively, it also exceeded GPT-3’s performance in few-shot scenarios on 10 of these datasets. They noted that instruction tuning shows significant effectiveness in tasks that are naturally expressed as instructions, such as natural language inference (NLI), question answering (QA), translation, and converting structures to text. However, it was less effective in tasks inherently structured as language modeling tasks, where instructions might be superfluous. These include commonsense reasoning and coreference resolution tasks typically presented as completing an unfinished sentence or paragraph.</p> <p>Now, instruction tuning <code class="language-plaintext highlighter-rouge">frames all tasks in the form of natural language instruction to natural language response mapping</code>.</p> <h2 id="reinforcement-learning-from-human-feedback-rlhf">Reinforcement Learning from Human Feedback (RLHF)</h2> <p>While instruction tuning is highly effective, it has inherent limitations. For a given input, the target is the single correct answer. This requires formalizing the correct behavior for a given input</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/instruction_tuning_limitation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/instruction_tuning_limitation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/instruction_tuning_limitation-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/instruction_tuning_limitation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Difficulty in formalizing the correct behavior for a given input. </div> <p>We are progressively aiming to impart more complex behaviors to models. The primary challenge in this process appears to be the objective function used in instruction fine-tuning. This maximum likelihood objective is a fixed function without adjustable parameters. A question arises: is it possible to add parameters to this objective function and learn it?</p> <p>One of the framework to solve this is RLHF (reinforcement learning human feedback).</p> <p>RLHF involves three key steps: collecting human feedback, fitting a reward model, and optimizing the policy with RL. Here, I introduce a straightforward formal framework for Reinforcement Learning from Human Feedback (RLHF), partially based on the framework proposed by Christiano et al. in 2017. <d-cite key="christiano2023deep"></d-cite></p> <p>The RLHF process begins with an optional pretraining step, where a base model $\pi_\theta$ with parameters $\theta$ generates a range of examples. In the context of language models, this often involves a language generator pretrained on web text or other datasets.</p> <p>The first active step involves collecting human feedback. Here, examples are derived from the base model, and feedback is gathered based on these examples. This involves a feedback function mapping each example and random noise to a feedback output, often modeled as</p> <p>$x_i \sim \pi_\theta, \quad y_i=f\left(\mathcal{H}, x_i, \epsilon_i\right)$</p> <p>For instance, in RLHF for chatbots, tasks may consist of conversational pairs with feedback expressed as preferences within these pairs.</p> <p>The second step is to fit a reward model $\hat{r}_\phi$ using the feedback, aiming to approximate human evaluations as accurately as possible. The training involves minimizing a loss function over a dataset of examples and preferences, often incorporating a regularizer.</p> <p>Given a dataset of examples and preferences $\mathcal{D}=\left{\left(x_i, y_i\right)_{i=1, \ldots, n}\right}$, the parameters $\phi$ are trained to minimize</p> <p>$\mathcal{L}(\mathcal{D}, \phi)=\sum_{i=1}^n \ell\left(\hat{r}_\phi\left(x_i\right), y_i\right)+\lambda_r(\phi)$</p> <p>$l$ represents an appropriate loss function and $\lambda_r$ acts as a regularizing term. For instance, when dealing with feedback in the form of pairwise comparisons, suitable loss functions might include the cross-entropy loss or the Bayesian personalized ranking loss.</p> <p>The final step involves optimizing the policy using reinforcement learning, where the base model is fine-tuned using the reward model $\hat{r}<em>\phi$. The new parameters $\theta</em>{\text {new }}$ are trained to maximize a reward function, which typically includes a regularizer like a divergence-based penalty.</p> <p>The benefits of RLHF include its ability to allow humans to convey objectives without the need to explicitly define a reward function. This approach helps in reducing reward hacking issues that often arise with hand-specified proxies, making the process of reward shaping more intuitive and implicit. Additionally, RLHF capitalizes on human judgments, which are frequently easier to provide than detailed demonstrations. This has led to its successful application in teaching policies complex solutions in various control environments and in fine-tuning large language models.</p> <p>However, RLHF does have its own challenges such as reward hacking. More problems can be found in Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. <d-cite key="casper2023open"></d-cite></p> <h2 id="training-language-models-to-follow-instructions-with-human-feedback-instruct-gpt">Training Language Models to follow instructions with Human Feedback (Instruct-GPT)</h2> <p>Now, putting everything together, OpenAI has trained language models that surpass GPT-3 in effectively adhering to user intentions and are also enhanced in truthfulness and reduced toxicity. This advancement is attributed to techniques derived from their alignment research. The InstructGPT models <d-cite key="ouyang2022training"></d-cite>, developed with human feedback integrated into the training process, have been implemented as the default models on their API.</p> <p>The effectiveness of InstructGPT in following user instructions was initially assessed by comparing its outputs with those from GPT-3, using labelers for the evaluation. The comparison revealed a clear preference for InstructGPT models, particularly on prompts given to both InstructGPT and GPT-3 models via the API. This preference remained consistent even when modifying the GPT-3 prompts with a prefix to enhance its instruction-following capabilities.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/instructgpt_result-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/instructgpt_result-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/instructgpt_result-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/instructgpt_result.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Quality ratings of model outputs on a 1–7 scale (y-axis), for various model sizes (x-axis), on prompts submitted to InstructGPT models on OpenAI API. InstructGPT outputs are given much higher scores by our labelers than outputs from GPT-3 with a few-shot prompt and without, as well as models fine-tuned with supervised learning. They find similar results for prompts submitted to GPT-3 models on the API. </div> <p>Initially, a dataset of human-authored demonstrations based on prompts from the API is compiled to establish supervised learning baselines. Following this, a larger dataset of human assessments, comparing pairs of model outputs on various API prompts, is gathered. A reward model (RM) is then trained on this dataset to foresee the output preference of the labelers. In the final step, this RM is utilized as a reward function to refine the GPT-3 policy, aiming to maximize this reward by employing the Proximal Policy Optimization (PPO) algorithm.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/from_transformer_to_chatgpt/instructgpt-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/from_transformer_to_chatgpt/instructgpt-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/from_transformer_to_chatgpt/instructgpt-1400.webp"/> <img src="/assets/img/from_transformer_to_chatgpt/instructgpt.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <d-footnote>Thanks to Nicola Dainese for the inspiration, and thanks to Michael Heinzer, Ralph Aeschimann for reading drafts of this.</d-footnote> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript">comments powered by giscus.</a></noscript> </div> </div> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
---
layout: distill
title: From Transformer to ChatGPT
description: going back in time in order to move forward
tags: distill formatting
giscus_comments: true
date: 2023-11-03

authors:
  - name: Hoanh Le

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Attention is all you need
    subsections:
       - name: Attention mechanism
       - name: Multi-head Attention
       - name: Self-attention
       - name: Transformer encoder
       - name: Transformer decoder
       - name: Positional encoding
  - name: Improving Language Understanding by Generative Pre-Training (GPT-1)
    subsections:
       - name: A new paradigm
       - name: Generative Pre-training
       - name: Byte-Pair Encoding (BPE) / Tokenization
       - name: Discriminative fine-tuning
       - name: The long-term vision
  - name: Masked Language Modeling Pre-Training
  - name: Language Models are Unsupervised Multitask Learners (GPT-2)
  - name: Scaling Laws for Neural Language Models
  - name: Language Models are Few-Shot Learners (GPT-3)
  - name: Instruction Tuning (FLAN)
  - name: Reinforcement Learning from Human Feedback (RLHF)
  - name: Training Language Models to follow instructions with Human Feedback (Instruct-GPT)


# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .l-gutter img {
    width: 150%; /* This makes the image take up the full width of its parent container */
    height: auto; /* This maintains the aspect ratio of the image */
    max-width: none; /* This overrides the max-width set by any .img-fluid class */
    margin: 0 0;
  }

---

## Attention is all you need

Since their introduction in 2017, transformers <d-cite key="vaswani2023attention"></d-cite> have revolutionised NLP, and now applications over all domains in Deep Learning.
Why is this significant: rather than relying on domain-specific architectures, we can now utilize a single architecture for every domain.

On a high level: transformers represents an efficient, optimizable, general-purpose neural network. They are designed to encode and decode sequences of data, allowing individual tokens to interact and communicate with one another.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/from_transformer_to_chatgpt/architecture.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Attention mechanism



The attention mechanism let us lets  us  model dependencies  between  words  regardless  of their position in the sequence. It was first introduced by Bahdanau et al. <d-cite key="bahdanau2016neural"></d-cite> where they found out
```markdown
a fixed-length vector is a bottleneck in improving the performance of this 
basic encoder–decoder architecture, and propose to extend this by allowing 
a model to automatically (soft-)search for parts of a source sentence that 
are relevant to predicting a target word, without having to form these 
parts as a hard segment explicitly. 
```

Basic attention mechanism in transformers:


$$
\begin{aligned}
o_i & =\sum_{j=1}^n \alpha_{i j} z_j \\
\alpha_{i j} & =\frac{\exp \left(z_j^{\top} h_i / \sqrt{d_k}\right)}{\sum_{j^{\prime}=1}^n \exp \left(z_{j^{\prime}}^{\top} h_i / \sqrt{d_k}\right)}
\end{aligned}
$$

<div class="l-gutter">
    {% include figure.html path="assets/img/from_transformer_to_chatgpt/attention_transformer.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>

$d_k$ is the dimensionality of the $z_j$ and $h_i$.

The authors call this scaled-dot 

We can think of the scaled dot-product attention as finding values $v_j=z_j$ with keys $k_j=z_j$ that are closest to query $q_i=h_i$. 
Re-writing the scaled dot-product attention using keys, values and query:

$$
\begin{aligned}
o_i=\sum_{j=1}^n \alpha_{i j} z_j & o_i=\sum_{j=1}^n \alpha_{i j} v_j \\
\\
\alpha_{i j}=\frac{\exp \left(z_j^{\top} h_i / \sqrt{d_k}\right)}{\sum_{j^{\prime}=1}^n \exp \left(z_{j^{\prime}}^{\top} h_i / \sqrt{d_k}\right)} & \alpha_{i j}=\frac{\exp \left(k_j^{\top} q_i / \sqrt{d_k}\right)}{\sum_{j^{\prime}=1}^n \exp \left(k_{j^{\prime}}^{\top} q_i / \sqrt{d_k}\right)}
\end{aligned}
$$




Scaled dot-product attention:

$$
\begin{aligned}
o_i & =\sum_{j=1}^n \alpha_{i j} v_j \\
\alpha_{i j} & =\frac{\exp \left(k_j^{\top} q_i / \sqrt{d_k}\right)}{\sum_{j^{\prime}=1}^n \exp \left(k_{j^{\prime}}^{\top} q_i / \sqrt{d_k}\right)}
\end{aligned}
$$

<div class="l-gutter">
    {% include figure.html path="assets/img/from_transformer_to_chatgpt/scaled_dot_product.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>


in the matrix form:


$$
\begin{aligned}
& \qquad \operatorname{attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{\top}}{\sqrt{d_k}}\right) V \\
& \text { with } V \in R^{m \times d_v}, Q \in R^{n \times d_k}, K \in R^{m \times d_k}
\end{aligned}
$$


### Multihead attention

Instead of doing a single scaled dot-product attention, the authors found it beneficial to project keys, queries and values into lower-dimensional spaces, perform scaled dot-product attention there and concatenate the outputs

$$
\begin{gathered}
\operatorname{head}_i=\operatorname{attention}\left(Q W_i^Q, K W_i^K, V W_i^V\right) \\
\\
\operatorname{MultiHead}(Q, K, V)=\text { Concat }\left(\operatorname{head}_1, \ldots, \text { head }_h\right) W^O \\
\\
V \in R^{m \times d_v}, Q \in R^{n \times d_k}, K \in R^{m \times d_k} \\
\\
\text { head }_i \in R^{n \times d_i}, \text { output } \in R^{n \times d_k} .
\end{gathered}
$$


<div class="l-gutter">
    {% include figure.html path="assets/img/from_transformer_to_chatgpt/multi_head_attention.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>


### Self-Attention


Attention is all you need: Use the same multi-head attention mechanism to convert inputs $\left(\mathbf{x}_1, \ldots, \mathbf{x}_n\right)$ into representations $\left(\mathbf{z}_1, \ldots, \mathbf{z}_n\right)$.


For simplicity, assume that we use scaled dot-product attention:

$$
z_i=\sum_{j=1}^n \alpha_{i j} x_j \quad \alpha_{i j}=\frac{\exp \left(x_j^{\top} x_i / \sqrt{d_k}\right)}{\sum_{j^{\prime}=1}^n \exp \left(x_{j^{\prime}}^{\top} x_i / \sqrt{d_k}\right)}
$$

<div class="l-gutter">
    {% include figure.html path="assets/img/from_transformer_to_chatgpt/self_attention.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>

Thus, we use vectors $x_i$ as keys, values and queries. This is called self-attention.
Advantage: The first position affects the representation in the last position (and vice versa) already after one layer! (think how many layers are needed for that in RNN or convolutional encoders).

### Transformer encoder

After self-attention, the representation in each position is
processed with a mini-MLP (Feed Forward block in the
figure). The encoder is a stack of multiple such blocks (each
block contains an attention module and a mini-MLP).
<div class="l-gutter">
    {% include figure.html path="assets/img/from_transformer_to_chatgpt/transformer_encoder.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>


### Transformer decoder

When predicting word $y_i$ we can use the preceding words $y_1$, ..., $y_{i-1}$ but not subsequent words $y_i$, ..., $y_m$. Again, attention-is-all-you-need idea: Use
self-attention as a building block of the decoder. We need to make sure that we do not use
subsequent inputs $y_i$, ..., $y_m$ when producing output $o_i$ at position $i$. This is done using masked self-attention
<div class="l-gutter">
    {% include figure.html path="assets/img/from_transformer_to_chatgpt/transformer_decoder.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>

For simplicity, assume that we use scaled dot-product attention:

$$
h_i=\sum_{j=1}^m \alpha_{i j} v_j \quad \alpha_{i j}=\frac{\exp \left(v_j^{\top} v_i / \sqrt{d_k}+m_{i j}\right)}{\sum_{j^{\prime}=1}^m \exp \left(v_{j^{\prime}}^{\top} v_i / \sqrt{d_k}+m_{i j^{\prime}}\right)}
$$

We want not to use subsequent positions 

$$v_{i+1}, \ldots, v_m$$ when computing output $h_i$. We can do that using attention masks $m_{i j}$:


$$
\begin{aligned}
& m_{i j}=0, \text { if } j \leq i \\
& m_{i j}=-\infty \text { and therefore } \alpha_{i j}=0, \text { if } j>i
\end{aligned}
$$


After self-attention and cross-attention, the representation in each position is processed with a mini-MLP (Feed Forward block in the figure). The decoder is a stack of multiple such blocks (each
block contains two attention modules and a mini-MLP). 

<div class="l-gutter">
    {% include figure.html path="assets/img/from_transformer_to_chatgpt/transformer_decoder_architecture.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>


### Positional encoding

Transformers use hard-coded (not learned) positional encoding:


$$
\begin{aligned}
& PE(p, 2 i) =\sin \left(p / 10000^{2 i / d}\right) \\
& PE(p, 2 i+1) =\cos \left(p / 10000^{2 i / d}\right)
\end{aligned}
$$


where $p$ is position, $i$ is the element of the encoding. This encoding has the same dimensionality $d$ as input/output embeddings.
This encoding has the same dimensionality d as input/output embeddings.


#### End note
```
Encoder-decoder architecture
Transformer layers with Multi-Head Self-Attention and Feed-Forward sublayers 
Transformer Decoder layers also have Encoder-Decoder Attention sublayer
N Encoder and Decoder layers (N=6 in the paper)
Attention uses queries and key-value pairs to produce weighted sums of the values 
Feed-Forward layers then process the inputs independently for better representations
Trained end-to-end for translating sequences
Superior performance over RNNs
```

If you have trouble understanding the model still, consider looking at this [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) blog post.


## Improving Language Understanding by Generative Pre-Training (GPT-1)

[//]: need to paraphase this

OpenAI introduced the first version of GPT (Generative Pre-Training) <d-cite key="radford2018improving"></d-cite>, which 
is a scalable, task-agnostic architecture, achieving state-of-the-art results on multiple diverse tasks.
The approach combines two existing ideas: transformers and unsupervised pre-training  <d-cite key="dai2015semisupervised"></d-cite>.
The results show that pairing supervised learning methods with unsupervised pre-training works very well, and if we improve the
performance of the model in the pre-training phase, it might become good enough to not require any fine-tuning on more diverse tasks.

### Introducing a new paradigm

>Most deep learning methods require substantial amounts of manually labeled data, 
which restricts their applicability in many domains that suffer from a dearth of annotated resources.



Which raises the following questions:
- Can we learn from raw text instead of pairs of translated sentences? 
- What objectives should we use?
- How to apply (or “transfer”) what we learn to the tasks we care? 
- Does it help in practice to model language in order to solve NLP tasks?

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/from_transformer_to_chatgpt/gpt_new_paradigm.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Introducing a new paradigm
</div>


### Generative Pre-Training

Use raw text as a stream of data
No extra structure assumed about the data, except the
sequential nature of characters, words and sentences Train to predict the next token in a sequence
Use Transformer, as its excellent for handling complex dependencies within sequences
Decoder-only Transformer: modify architecture by removing the encoder and the encoder-decoder attention

<div class="l-gutter">
    {% include figure.html path="assets/img/from_transformer_to_chatgpt/generative_pre_training.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>


### Byte-Pair Encoding (BPE) / Tokenization

Good characteristics of good vocabulary
- Be able to represent ALL characters and symbols that exist in digital format
- The vocabulary should be as expressive as possible (e.g. words are more expressive than characters)




## Equations

This theme supports rendering beautiful math in inline and display modes using [MathJax 3](https://www.mathjax.org/) engine.
You just need to surround your math expression with `$$`, like `$$ E = mc^2 $$`.
If you leave it inside a paragraph, it will produce an inline expression, just like $$ E = mc^2 $$.

To use display mode, again surround your expression with `$$` and place it as a separate paragraph.
Here is an example:

$$
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
$$

Note that MathJax 3 is [a major re-write of MathJax](https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html) that brought a significant improvement to the loading and rendering speed, which is now [on par with KaTeX](http://www.intmath.com/cg5/katex-mathjax-comparison.php).

***

## Citations

Citations are then used in the article body with the `<d-cite>` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.

Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.

***

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>

***

## Code Blocks

Syntax highlighting is provided within `<d-code>` tags.
An example of inline code snippets: `<d-code language="html">let x = 10;</d-code>`.
For larger blocks of code, add a `block` attribute:

<d-code block language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

**Note:** `<d-code>` blocks do not look good in the dark mode.
You can always use the default code-highlight using the `highlight` liquid tag:

{% highlight javascript %}
var x = 25;
function(x) {
  return x * x;
}
{% endhighlight %}

***

## Interactive Plots

You can add interative plots using plotly + iframes :framed_picture:

<div class="l-page">
  <iframe src="{{ '/assets/plotly/demo.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:

{% highlight python %}
import pandas as pd
import plotly.express as px
df = pd.read_csv(
  'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'
)
fig = px.density_mapbox(
  df,
  lat='Latitude',
  lon='Longitude',
  z='Magnitude',
  radius=10,
  center=dict(lat=0, lon=180),
  zoom=0,
  mapbox_style="stamen-terrain",
)
fig.show()
fig.write_html('assets/plotly/demo.html')
{% endhighlight %}

***

## Details boxes

Details boxes are collapsible boxes which hide additional information from the user. They can be added with the `details` liquid tag:

{% details Click here to know more %}
Additional details, where math $$ 2x - 1 $$ and `code` is rendered correctly.
{% enddetails %}

***

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

***

## Other Typography?

Emphasis, aka italics, with *asterisks* (`*asterisks*`) or _underscores_ (`_underscores_`).

Strong emphasis, aka bold, with **asterisks** or __underscores__.

Combined emphasis with **asterisks and _underscores_**.

Strikethrough uses two tildes. ~~Scratch this.~~

1. First ordered list item
2. Another item
⋅⋅* Unordered sub-list.
1. Actual numbers don't matter, just that it's a number
⋅⋅1. Ordered sub-list
4. And another item.

⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).

⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)

* Unordered list can use asterisks
- Or minuses
+ Or pluses

[I'm an inline-style link](https://www.google.com)

[I'm an inline-style link with title](https://www.google.com "Google's Homepage")

[I'm a reference-style link][Arbitrary case-insensitive reference text]

[I'm a relative reference to a repository file](../blob/master/LICENSE)

[You can use numbers for reference-style link definitions][1]

Or leave it empty and use the [link text itself].

URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <http://www.example.com> and sometimes
example.com (but not on Github, for example).

Some text to show that the reference links can follow later.

[arbitrary case-insensitive reference text]: https://www.mozilla.org
[1]: http://slashdot.org
[link text itself]: http://www.reddit.com

Here's our logo (hover to see the title text):

Inline-style:
![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 1")

Reference-style:
![alt text][logo]

[logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 2"

Inline `code` has `back-ticks around` it.

```javascript
var s = "JavaScript syntax highlighting";
alert(s);
```

```python
s = "Python syntax highlighting"
print s
```

```
No language indicated, so no syntax highlighting.
But let's throw in a <b>tag</b>.
```

Colons can be used to align columns.

| Tables        | Are           | Cool  |
| ------------- |:-------------:| -----:|
| col 3 is      | right-aligned | $1600 |
| col 2 is      | centered      |   $12 |
| zebra stripes | are neat      |    $1 |

There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don't need to make the
raw Markdown line up prettily. You can also use inline Markdown.

Markdown | Less | Pretty
--- | --- | ---
*Still* | `renders` | **nicely**
1 | 2 | 3

> Blockquotes are very handy in email to emulate reply text.
> This line is part of the same quote.

Quote break.

> This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote.


Here's a line for us to start with.

This line is separated from the one above by two newlines, so it will be a *separate paragraph*.

This line is also a separate paragraph, but...
This line is only separated by a single newline, so it's a separate line in the *same paragraph*.
